import os
import sys
import json
import torch
import cv2
import numpy as np
from ultralytics import YOLO
from ultralytics.utils.metrics import DetMetrics, box_iou
from collections import defaultdict, Counter
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report,
    top_k_accuracy_score,
    balanced_accuracy_score,
)
import pandas as pd
from datetime import datetime
import glob
from tqdm.auto import tqdm
import warnings

warnings.filterwarnings("ignore")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.abspath(os.path.join(__file__, "..", "..")))

# ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
from config.config import CLASS_MAPPINGS

# ê¸°ë³¸ ì„¤ì •
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
IMAGE_DIR = r"D:\zeroback_KHJ_end\zeroback-project\backend\DATA\images"
LABEL_DIR = r"D:\zeroback_KHJ_end\zeroback-project\backend\DATA\labels"
MODEL_PATH = r"D:\zeroback_KHJ_end\zeroback-project\backend\models\YOLOv11_large.pt"
RESULTS_DIR = r"D:\zeroback_KHJ_end\zeroback-project\AI\YOLOv11_summary\RESULTS"

# 21ê°œ ì¹´í…Œê³ ë¦¬ ì •ì˜
CATEGORIES = CLASS_MAPPINGS["category"]
NUM_CLASSES = len(CATEGORIES)

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘: ì„¸ë¶€ ì¹´í…Œê³ ë¦¬ â†’ ëŒ€ë¶„ë¥˜
CATEGORY_GROUPS = {
    "ìƒì˜": ["íƒ‘", "ë¸”ë¼ìš°ìŠ¤", "í‹°ì…”ì¸ ", "ë‹ˆíŠ¸ì›¨ì–´", "ì…”ì¸ ", "ë¸Œë¼íƒ‘", "í›„ë“œí‹°"],
    "í•˜ì˜": ["ì²­ë°”ì§€", "íŒ¬ì¸ ", "ìŠ¤ì»¤íŠ¸", "ë ˆê¹…ìŠ¤", "ì¡°ê±°íŒ¬ì¸ "],
    "ì•„ìš°í„°": ["ì½”íŠ¸", "ì¬í‚·", "ì í¼", "íŒ¨ë”©", "ë² ìŠ¤íŠ¸", "ê°€ë””ê±´", "ì§šì—…"],
    "ì›í”¼ìŠ¤": ["ë“œë ˆìŠ¤", "ì í”„ìˆ˜íŠ¸"],
}

# í•œê¸€â†’ì˜ì–´ í‚¤ ë§µí•‘
KOR_TO_ENG = {
    "ì¹´í…Œê³ ë¦¬": "category",
    "ìƒ‰ìƒ": "color",
    "ì†Œë§¤ê¸°ì¥": "sleeve_length",
    "ê¸°ì¥": "sleeve_length",
    "ì˜·ê¹ƒ": "collar",
    "ë„¥ë¼ì¸": "neckline",
    "í•": "fit",
    "í”„ë¦°íŠ¸": "print",
    "ì†Œì¬": "material",
    "ë””í…Œì¼": "detail",
    "ìŠ¤íƒ€ì¼": "style",
}


class YOLODetectionValidator:
    """YOLO ê°ì²´ íƒì§€ + ë¶„ë¥˜ ì„±ëŠ¥ ê²€ì¦ í´ë˜ìŠ¤ (ì‹¤ì œ mAP í¬í•¨)"""

    def __init__(self, model_path, device=None):
        self.device = device or DEVICE
        self.model_path = model_path

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")

        # YOLO ëª¨ë¸ ë¡œë“œ
        self.model = YOLO(model_path)
        print(f"âœ… YOLO ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")

        # mAP ê³„ì‚°ì„ ìœ„í•œ ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        self.det_metrics = DetMetrics()

    def extract_bbox_from_json(self, json_data, img_width=800, img_height=800):
        """JSONì—ì„œ ë°”ìš´ë”© ë°•ìŠ¤ ì •ë³´ ì¶”ì¶œ"""
        bboxes = []

        try:
            rect_coords = (
                json_data.get("ë°ì´í„°ì…‹ ì •ë³´", {})
                .get("ë°ì´í„°ì…‹ ìƒì„¸ì„¤ëª…", {})
                .get("ë ‰íŠ¸ì¢Œí‘œ", {})
            )
            labeling_info = (
                json_data.get("ë°ì´í„°ì…‹ ì •ë³´", {})
                .get("ë°ì´í„°ì…‹ ìƒì„¸ì„¤ëª…", {})
                .get("ë¼ë²¨ë§", {})
            )

            # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì²˜ë¦¬
            for group_name in ["ìƒì˜", "í•˜ì˜", "ì•„ìš°í„°", "ì›í”¼ìŠ¤"]:
                if group_name in rect_coords and rect_coords[group_name]:
                    for bbox_info in rect_coords[group_name]:
                        if not bbox_info:  # ë¹ˆ ë”•ì…”ë„ˆë¦¬ ìŠ¤í‚µ
                            continue

                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ì¶”ì¶œ
                        x = bbox_info.get("Xì¢Œí‘œ", 0)
                        y = bbox_info.get("Yì¢Œí‘œ", 0)
                        w = bbox_info.get("ê°€ë¡œ", 0)
                        h = bbox_info.get("ì„¸ë¡œ", 0)

                        if w > 0 and h > 0:  # ìœ íš¨í•œ ë°•ìŠ¤ë§Œ
                            # ë¼ë²¨ë§ì—ì„œ í•´ë‹¹ ê·¸ë£¹ì˜ ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
                            if (
                                group_name in labeling_info
                                and labeling_info[group_name]
                            ):
                                for item in labeling_info[group_name]:
                                    if isinstance(item, dict) and "ì¹´í…Œê³ ë¦¬" in item:
                                        category = item["ì¹´í…Œê³ ë¦¬"]
                                        if category in CATEGORIES:
                                            class_id = CATEGORIES.index(category)

                                            # YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (x_center, y_center, width, height) - ì •ê·œí™”
                                            x_center = (x + w / 2) / img_width
                                            y_center = (y + h / 2) / img_height
                                            norm_w = w / img_width
                                            norm_h = h / img_height

                                            bboxes.append(
                                                {
                                                    "class_id": class_id,
                                                    "category": category,
                                                    "group": group_name,
                                                    "bbox_norm": [
                                                        x_center,
                                                        y_center,
                                                        norm_w,
                                                        norm_h,
                                                    ],  # ì •ê·œí™”ëœ ì¢Œí‘œ
                                                    "bbox_pixel": [
                                                        x,
                                                        y,
                                                        x + w,
                                                        y + h,
                                                    ],  # í”½ì…€ ì¢Œí‘œ (x1, y1, x2, y2)
                                                    "area": w * h,
                                                }
                                            )
                                        break
        except Exception as e:
            print(f"âš ï¸ ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ ì˜¤ë¥˜: {e}")

        return bboxes

    def load_ground_truth_data(self, image_dir, label_dir):
        """JSON íŒŒì¼ì—ì„œ ground truth ë°ì´í„° ë¡œë“œ (ë°”ìš´ë”© ë°•ìŠ¤ + ë¼ë²¨)"""
        gt_data = {}
        image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(".jpg")]

        print(f"ğŸ“ ì´ë¯¸ì§€ í´ë”ì—ì„œ {len(image_files)}ê°œ íŒŒì¼ ë°œê²¬")

        valid_count = 0
        for img_file in tqdm(image_files, desc="Ground Truth ë°ì´í„° ë¡œë”© ì¤‘"):
            json_file = os.path.join(label_dir, img_file.rsplit(".", 1)[0] + ".json")

            if not os.path.exists(json_file):
                continue

            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)

                # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´
                img_info = data.get("ì´ë¯¸ì§€ ì •ë³´", {})
                img_width = img_info.get("ì´ë¯¸ì§€ ë„ˆë¹„", 800)
                img_height = img_info.get("ì´ë¯¸ì§€ ë†’ì´", 800)

                # ë°”ìš´ë”© ë°•ìŠ¤ ì¶”ì¶œ
                bboxes = self.extract_bbox_from_json(data, img_width, img_height)

                if bboxes:
                    gt_data[img_file] = {
                        "bboxes": bboxes,
                        "img_width": img_width,
                        "img_height": img_height,
                        "num_objects": len(bboxes),
                    }
                    valid_count += 1

            except Exception as e:
                print(f"âš ï¸ JSON íŒŒì¼ ë¡œë”© ì˜¤ë¥˜ ({img_file}): {e}")
                continue

        print(f"âœ… ìœ íš¨í•œ Ground Truth ë°ì´í„° {valid_count}ê°œ ë¡œë“œ ì™„ë£Œ")
        return gt_data

    def predict_with_boxes(
        self, image_dir, gt_data, conf_threshold=0.5, iou_threshold=0.45
    ):
        """YOLOë¡œ ê°ì²´ íƒì§€ + ë¶„ë¥˜ ì˜ˆì¸¡ ìˆ˜í–‰"""
        predictions = {}

        print(f"ğŸ” {len(gt_data)}ê°œ ì´ë¯¸ì§€ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")

        for img_file in tqdm(gt_data.keys(), desc="YOLO ì˜ˆì¸¡ ì¤‘"):
            img_path = os.path.join(image_dir, img_file)

            try:
                # YOLO ì˜ˆì¸¡ ìˆ˜í–‰
                results = self.model(
                    img_path, conf=conf_threshold, iou=iou_threshold, verbose=False
                )

                if len(results) == 0 or len(results[0].boxes) == 0:
                    predictions[img_file] = {
                        "boxes": [],
                        "confidences": [],
                        "classes": [],
                        "num_detections": 0,
                    }
                    continue

                # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ
                boxes = results[0].boxes
                pred_boxes = boxes.xyxy.cpu().numpy()  # x1, y1, x2, y2 í˜•ì‹
                confidences = boxes.conf.cpu().numpy()
                classes = boxes.cls.cpu().numpy().astype(int)

                predictions[img_file] = {
                    "boxes": pred_boxes.tolist(),
                    "confidences": confidences.tolist(),
                    "classes": classes.tolist(),
                    "num_detections": len(pred_boxes),
                }

            except Exception as e:
                print(f"âš ï¸ ì˜ˆì¸¡ ì˜¤ë¥˜ ({img_file}): {e}")
                predictions[img_file] = {
                    "boxes": [],
                    "confidences": [],
                    "classes": [],
                    "num_detections": 0,
                }
                continue

        return predictions

    def calculate_map_metrics(
        self, gt_data, predictions, iou_thresholds=[0.5, 0.75], conf_threshold=0.5
    ):
        """ì‹¤ì œ mAP ë° ê´€ë ¨ ë©”íŠ¸ë¦­ ê³„ì‚° (AP ê³¡ì„  ê¸°ë°˜)"""
        print("ğŸ“Š ì‹¤ì œ mAP ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")

        # ëª¨ë“  ì˜ˆì¸¡ì„ confidence ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìˆ˜ì§‘
        all_detections = []  # [(confidence, class_id, bbox, img_file, pred_idx)]

        # í´ë˜ìŠ¤ë³„ GT ê°œìˆ˜ ê³„ì‚°
        class_gt_counts = {i: 0 for i in range(NUM_CLASSES)}

        for img_file in gt_data.keys():
            if img_file not in predictions:
                continue

            gt_info = gt_data[img_file]
            pred_info = predictions[img_file]

            # GT ê°œìˆ˜ ì¹´ìš´íŠ¸
            for bbox_info in gt_info["bboxes"]:
                class_gt_counts[bbox_info["class_id"]] += 1

            # ì˜ˆì¸¡ ë°•ìŠ¤ë“¤ì„ confidence ìˆœìœ¼ë¡œ ì²˜ë¦¬
            for i, (pred_bbox, pred_conf, pred_class) in enumerate(
                zip(pred_info["boxes"], pred_info["confidences"], pred_info["classes"])
            ):
                if pred_conf < conf_threshold:
                    continue

                all_detections.append(
                    {
                        "confidence": pred_conf,
                        "class_id": pred_class,
                        "bbox": pred_bbox,
                        "img_file": img_file,
                        "pred_idx": i,
                    }
                )

        # Confidence ìˆœìœ¼ë¡œ ì •ë ¬
        all_detections.sort(key=lambda x: x["confidence"], reverse=True)

        map_metrics = {}

        for iou_thresh in iou_thresholds:
            print(f"  IoU Threshold {iou_thresh} ì²˜ë¦¬ ì¤‘...")

            # í´ë˜ìŠ¤ë³„ AP ê³„ì‚°
            class_aps = {}
            class_metrics = {}
            total_tp, total_fp, total_fn = 0, 0, 0

            for class_id in range(NUM_CLASSES):
                category_name = CATEGORIES[class_id]

                # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì˜ˆì¸¡ë“¤ë§Œ í•„í„°ë§
                class_detections = [
                    d for d in all_detections if d["class_id"] == class_id
                ]

                if len(class_detections) == 0:
                    # ì˜ˆì¸¡ì´ ì—†ëŠ” ê²½ìš°
                    class_aps[category_name] = 0.0
                    class_metrics[category_name] = {
                        "precision": 0.0,
                        "recall": 0.0,
                        "ap": 0.0,
                        "tp": 0,
                        "fp": 0,
                        "fn": class_gt_counts[class_id],
                    }
                    total_fn += class_gt_counts[class_id]
                    continue

                # AP ê³„ì‚°ì„ ìœ„í•œ TP/FP íŒì •
                tp_list = []
                fp_list = []
                matched_gt = set()  # ì´ë¯¸ ë§¤ì¹­ëœ GT ë°•ìŠ¤ë“¤

                for detection in class_detections:
                    img_file = detection["img_file"]
                    pred_bbox = detection["bbox"]

                    if img_file not in gt_data:
                        fp_list.append(1)
                        tp_list.append(0)
                        continue

                    gt_info = gt_data[img_file]

                    # í•´ë‹¹ í´ë˜ìŠ¤ì˜ GT ë°•ìŠ¤ë“¤
                    gt_boxes = []
                    gt_indices = []
                    for idx, bbox_info in enumerate(gt_info["bboxes"]):
                        if bbox_info["class_id"] == class_id:
                            gt_boxes.append(bbox_info["bbox_pixel"])
                            gt_indices.append(f"{img_file}_{idx}")

                    if len(gt_boxes) == 0:
                        # í•´ë‹¹ í´ë˜ìŠ¤ì˜ GTê°€ ì—†ëŠ” ê²½ìš°
                        fp_list.append(1)
                        tp_list.append(0)
                        continue

                    # IoU ê³„ì‚°í•˜ì—¬ ìµœì  ë§¤ì¹­ ì°¾ê¸°
                    ious = self._calculate_iou_matrix(
                        np.array([pred_bbox]), np.array(gt_boxes)
                    )[
                        0
                    ]  # ì²« ë²ˆì§¸ í–‰ë§Œ ì‚¬ìš©

                    best_iou_idx = np.argmax(ious)
                    best_iou = ious[best_iou_idx]
                    best_gt_key = gt_indices[best_iou_idx]

                    if best_iou >= iou_thresh and best_gt_key not in matched_gt:
                        # True Positive
                        tp_list.append(1)
                        fp_list.append(0)
                        matched_gt.add(best_gt_key)
                    else:
                        # False Positive
                        tp_list.append(0)
                        fp_list.append(1)

                # AP ê³„ì‚° (Precision-Recall ê³¡ì„ ì˜ ë©´ì )
                tp_array = np.array(tp_list)
                fp_array = np.array(fp_list)

                # ëˆ„ì í•© ê³„ì‚°
                tp_cumsum = np.cumsum(tp_array)
                fp_cumsum = np.cumsum(fp_array)

                # Precisionê³¼ Recall ê³„ì‚°
                precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-6)
                recall = tp_cumsum / (class_gt_counts[class_id] + 1e-6)

                # AP ê³„ì‚° (11-point interpolation ë°©ì‹)
                ap = self._calculate_ap_11point(precision, recall)

                # ìµœì¢… ë©”íŠ¸ë¦­
                final_tp = int(tp_cumsum[-1]) if len(tp_cumsum) > 0 else 0
                final_fp = int(fp_cumsum[-1]) if len(fp_cumsum) > 0 else 0
                final_fn = class_gt_counts[class_id] - final_tp

                final_precision = (
                    final_tp / (final_tp + final_fp)
                    if (final_tp + final_fp) > 0
                    else 0.0
                )
                final_recall = (
                    final_tp / (final_tp + final_fn)
                    if (final_tp + final_fn) > 0
                    else 0.0
                )

                class_aps[category_name] = ap
                class_metrics[category_name] = {
                    "precision": final_precision,
                    "recall": final_recall,
                    "ap": ap,
                    "tp": final_tp,
                    "fp": final_fp,
                    "fn": final_fn,
                }

                total_tp += final_tp
                total_fp += final_fp
                total_fn += final_fn

            # ì „ì²´ ì„±ëŠ¥ ê³„ì‚°
            overall_precision = (
                total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
            )
            overall_recall = (
                total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
            )
            overall_f1 = (
                2
                * overall_precision
                * overall_recall
                / (overall_precision + overall_recall)
                if (overall_precision + overall_recall) > 0
                else 0
            )

            # í‰ê·  AP (mAP) ê³„ì‚°
            mean_ap = np.mean(list(class_aps.values()))

            map_metrics[f"iou_{iou_thresh}"] = {
                "precision": overall_precision,
                "recall": overall_recall,
                "f1_score": overall_f1,
                "mean_ap": mean_ap,  # ì‹¤ì œ mAP ê°’
                "tp": total_tp,
                "fp": total_fp,
                "fn": total_fn,
                "class_metrics": class_metrics,
                "class_aps": class_aps,  # í´ë˜ìŠ¤ë³„ AP ê°’ë“¤
            }

        # ì „ì²´ mAP ê°’ë“¤
        map_50 = map_metrics.get("iou_0.5", {}).get("mean_ap", 0)
        map_75 = map_metrics.get("iou_0.75", {}).get("mean_ap", 0)
        map_50_95 = np.mean([map_50, map_75])  # ê·¼ì‚¬ê°’

        return {
            "map_metrics": map_metrics,
            "map_50": map_50,
            "map_75": map_75,
            "map_50_95": map_50_95,
            "total_gt_objects": sum(class_gt_counts.values()),
            "total_pred_objects": len(all_detections),
            "total_images": len(gt_data),
        }

    def _calculate_ap_11point(self, precision, recall):
        """11-point interpolation ë°©ì‹ìœ¼ë¡œ AP ê³„ì‚°"""
        # 11ê°œ recall ì§€ì ì—ì„œì˜ precision ê°’ë“¤
        recall_points = np.linspace(0, 1, 11)
        interpolated_precisions = []

        for r in recall_points:
            # í˜„ì¬ recall ì§€ì ë³´ë‹¤ í° recallë“¤ ì¤‘ì—ì„œ ìµœëŒ€ precision ì°¾ê¸°
            precisions_at_recall = precision[recall >= r]
            if len(precisions_at_recall) == 0:
                interpolated_precisions.append(0)
            else:
                interpolated_precisions.append(np.max(precisions_at_recall))

        # í‰ê·  ê³„ì‚°
        ap = np.mean(interpolated_precisions)
        return ap

    def _calculate_iou_matrix(self, boxes1, boxes2):
        """ë‘ ë°•ìŠ¤ ì§‘í•© ê°„ì˜ IoU í–‰ë ¬ ê³„ì‚°"""
        if len(boxes1) == 0 or len(boxes2) == 0:
            return np.zeros((len(boxes1), len(boxes2)))

        # êµì§‘í•© ì˜ì—­ ê³„ì‚°
        x1 = np.maximum(boxes1[:, None, 0], boxes2[None, :, 0])
        y1 = np.maximum(boxes1[:, None, 1], boxes2[None, :, 1])
        x2 = np.minimum(boxes1[:, None, 2], boxes2[None, :, 2])
        y2 = np.minimum(boxes1[:, None, 3], boxes2[None, :, 3])

        intersection = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)

        # í•©ì§‘í•© ì˜ì—­ ê³„ì‚°
        area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])
        area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])
        union = area1[:, None] + area2[None, :] - intersection

        # IoU ê³„ì‚°
        iou = intersection / (union + 1e-6)
        return iou

    def calculate_classification_metrics(self, gt_data, predictions):
        """ë¶„ë¥˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° (ê¸°ì¡´ ë°©ì‹)"""
        # ë¶„ë¥˜ ì„±ëŠ¥ì„ ìœ„í•´ ê°€ì¥ ë†’ì€ confidenceì˜ ì˜ˆì¸¡ë§Œ ì‚¬ìš©
        y_true = []
        y_pred = []
        confidences = []

        for img_file in gt_data.keys():
            if img_file not in predictions:
                continue

            gt_info = gt_data[img_file]
            pred_info = predictions[img_file]

            # GTì—ì„œ ê°€ì¥ í° ê°ì²´ì˜ í´ë˜ìŠ¤ (ëŒ€í‘œ í´ë˜ìŠ¤ë¡œ ì‚¬ìš©)
            if not gt_info["bboxes"]:
                continue

            largest_gt = max(gt_info["bboxes"], key=lambda x: x["area"])
            gt_class = largest_gt["class_id"]

            # ì˜ˆì¸¡ì—ì„œ ê°€ì¥ ë†’ì€ confidenceì˜ í´ë˜ìŠ¤
            if pred_info["num_detections"] == 0:
                continue

            best_idx = np.argmax(pred_info["confidences"])
            pred_class = pred_info["classes"][best_idx]
            confidence = pred_info["confidences"][best_idx]

            y_true.append(gt_class)
            y_pred.append(pred_class)
            confidences.append(confidence)

        if not y_true:
            return None

        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        confidences = np.array(confidences)

        # ê¸°ë³¸ ë¶„ë¥˜ ë©”íŠ¸ë¦­ë“¤
        accuracy = accuracy_score(y_true, y_pred)
        balanced_acc = balanced_accuracy_score(y_true, y_pred)

        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
            y_true, y_pred, average="macro", zero_division=0
        )
        precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(
            y_true, y_pred, average="micro", zero_division=0
        )
        precision_weighted, recall_weighted, f1_weighted, _ = (
            precision_recall_fscore_support(
                y_true, y_pred, average="weighted", zero_division=0
            )
        )

        # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­
        precision_per_class, recall_per_class, f1_per_class, support_per_class = (
            precision_recall_fscore_support(
                y_true, y_pred, average=None, zero_division=0
            )
        )

        # í´ë˜ìŠ¤ ë¶„í¬
        gt_class_counts = Counter(y_true)
        pred_class_counts = Counter(y_pred)

        # Confusion Matrix
        cm = confusion_matrix(y_true, y_pred, labels=range(NUM_CLASSES))

        return {
            "classification_metrics": {
                "accuracy": float(accuracy),
                "balanced_accuracy": float(balanced_acc),
                "macro_precision": float(precision_macro),
                "macro_recall": float(recall_macro),
                "macro_f1": float(f1_macro),
                "micro_precision": float(precision_micro),
                "micro_recall": float(recall_micro),
                "micro_f1": float(f1_micro),
                "weighted_precision": float(precision_weighted),
                "weighted_recall": float(recall_weighted),
                "weighted_f1": float(f1_weighted),
            },
            "confidence_stats": {
                "mean_confidence": float(np.mean(confidences)),
                "std_confidence": float(np.std(confidences)),
                "min_confidence": float(np.min(confidences)),
                "max_confidence": float(np.max(confidences)),
                "median_confidence": float(np.median(confidences)),
            },
            "class_wise_metrics": {
                CATEGORIES[i]: {
                    "precision": (
                        float(precision_per_class[i])
                        if i < len(precision_per_class)
                        else 0.0
                    ),
                    "recall": (
                        float(recall_per_class[i]) if i < len(recall_per_class) else 0.0
                    ),
                    "f1_score": (
                        float(f1_per_class[i]) if i < len(f1_per_class) else 0.0
                    ),
                    "support": (
                        int(support_per_class[i]) if i < len(support_per_class) else 0
                    ),
                    "gt_count": int(gt_class_counts.get(i, 0)),
                    "pred_count": int(pred_class_counts.get(i, 0)),
                }
                for i in range(NUM_CLASSES)
            },
            "class_distribution": {
                "ground_truth": {
                    CATEGORIES[i]: int(gt_class_counts.get(i, 0))
                    for i in range(NUM_CLASSES)
                },
                "predictions": {
                    CATEGORIES[i]: int(pred_class_counts.get(i, 0))
                    for i in range(NUM_CLASSES)
                },
            },
            "confusion_matrix": cm.tolist(),
            "total_samples": len(y_true),
        }

    def save_comprehensive_results(
        self, map_results, classification_results, predictions, gt_data, output_dir
    ):
        """ì¢…í•© ê²°ê³¼ ì €ì¥ (ê°œì„ ëœ CSV í¬í•¨, AP ê°’ ì¶”ê°€)"""
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ì „ì²´ ë©”íŠ¸ë¦­ í†µí•©
        comprehensive_metrics = {
            "overview": {
                "model_path": self.model_path,
                "total_images": len(gt_data),
                "total_classes": NUM_CLASSES,
                "class_names": CATEGORIES.copy(),
                "evaluation_timestamp": timestamp,
            },
            "detection_metrics": map_results,
            "classification_metrics": classification_results,
        }

        # 1. ì¢…í•© ë©”íŠ¸ë¦­ JSON ì €ì¥
        metrics_path = os.path.join(
            output_dir, f"comprehensive_yolo_metrics_{timestamp}.json"
        )
        with open(metrics_path, "w", encoding="utf-8") as f:
            json.dump(comprehensive_metrics, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ ì¢…í•© ë©”íŠ¸ë¦­ ì €ì¥: {metrics_path}")

        # 2. ìƒì„¸ ì˜ˆì¸¡ ê²°ê³¼ CSV (ê¸°ì¡´ ìœ ì§€)
        detailed_results = []
        for img_file in gt_data.keys():
            if img_file not in predictions:
                continue

            gt_info = gt_data[img_file]
            pred_info = predictions[img_file]

            # ê° GT ê°ì²´ì— ëŒ€í•´
            for gt_bbox in gt_info["bboxes"]:
                row = {
                    "image_file": img_file,
                    "gt_category": gt_bbox["category"],
                    "gt_class_id": gt_bbox["class_id"],
                    "gt_group": gt_bbox["group"],
                    "gt_bbox_x1": gt_bbox["bbox_pixel"][0],
                    "gt_bbox_y1": gt_bbox["bbox_pixel"][1],
                    "gt_bbox_x2": gt_bbox["bbox_pixel"][2],
                    "gt_bbox_y2": gt_bbox["bbox_pixel"][3],
                    "gt_area": gt_bbox["area"],
                    "num_predictions": pred_info["num_detections"],
                }

                # ê°€ì¥ ê°€ê¹Œìš´/ê²¹ì¹˜ëŠ” ì˜ˆì¸¡ ì°¾ê¸°
                if pred_info["num_detections"] > 0:
                    best_match_idx = 0
                    if pred_info["confidences"]:
                        best_match_idx = np.argmax(pred_info["confidences"])

                    row.update(
                        {
                            "pred_category": (
                                CATEGORIES[pred_info["classes"][best_match_idx]]
                                if pred_info["classes"][best_match_idx] < NUM_CLASSES
                                else "unknown"
                            ),
                            "pred_class_id": pred_info["classes"][best_match_idx],
                            "pred_confidence": pred_info["confidences"][best_match_idx],
                            "pred_bbox_x1": pred_info["boxes"][best_match_idx][0],
                            "pred_bbox_y1": pred_info["boxes"][best_match_idx][1],
                            "pred_bbox_x2": pred_info["boxes"][best_match_idx][2],
                            "pred_bbox_y2": pred_info["boxes"][best_match_idx][3],
                            "classification_correct": gt_bbox["class_id"]
                            == pred_info["classes"][best_match_idx],
                        }
                    )
                else:
                    row.update(
                        {
                            "pred_category": "no_detection",
                            "pred_class_id": -1,
                            "pred_confidence": 0.0,
                            "pred_bbox_x1": 0,
                            "pred_bbox_y1": 0,
                            "pred_bbox_x2": 0,
                            "pred_bbox_y2": 0,
                            "classification_correct": False,
                        }
                    )

                detailed_results.append(row)

        if detailed_results:
            detailed_df = pd.DataFrame(detailed_results)
            detailed_csv_path = os.path.join(
                output_dir, f"detailed_detection_results_{timestamp}.csv"
            )
            detailed_df.to_csv(detailed_csv_path, index=False, encoding="utf-8-sig")
            print(f"ğŸ“Š ìƒì„¸ íƒì§€ ê²°ê³¼ ì €ì¥: {detailed_csv_path}")

        # 3. ê°œì„ ëœ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ìš”ì•½ CSV (ëª¨ë“  21ê°œ ì¹´í…Œê³ ë¦¬ í¬í•¨, AP ê°’ ì¶”ê°€)
        if map_results and "map_metrics" in map_results:
            category_summary = []

            # ëª¨ë“  21ê°œ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ì²˜ë¦¬
            for category in CATEGORIES:
                category_data = {
                    "category": category,
                    "category_id": CATEGORIES.index(category),
                    "group": None,
                }

                # ê·¸ë£¹ ì •ë³´ ì¶”ê°€
                for group_name, categories in CATEGORY_GROUPS.items():
                    if category in categories:
                        category_data["group"] = group_name
                        break

                # IoUë³„ ì„±ëŠ¥ ì¶”ê°€ (AP ê°’ í¬í•¨)
                for iou_key, iou_data in map_results["map_metrics"].items():
                    iou_threshold = iou_key.replace("iou_", "")
                    class_metrics = iou_data.get("class_metrics", {})

                    if category in class_metrics:
                        metrics = class_metrics[category]
                    else:
                        metrics = {
                            "precision": 0.0,
                            "recall": 0.0,
                            "ap": 0.0,
                            "tp": 0,
                            "fp": 0,
                            "fn": 0,
                        }

                    # F1-Score ê³„ì‚°
                    f1_score = (
                        2
                        * metrics["precision"]
                        * metrics["recall"]
                        / (metrics["precision"] + metrics["recall"])
                        if (metrics["precision"] + metrics["recall"]) > 0
                        else 0
                    )

                    # IoUë³„ ë©”íŠ¸ë¦­ ì¶”ê°€ (AP ê°’ í¬í•¨)
                    category_data[f"ap_iou_{iou_threshold}"] = round(
                        metrics["ap"], 4
                    )  # **AP ê°’ ì¶”ê°€**
                    category_data[f"precision_iou_{iou_threshold}"] = round(
                        metrics["precision"], 4
                    )
                    category_data[f"recall_iou_{iou_threshold}"] = round(
                        metrics["recall"], 4
                    )
                    category_data[f"f1_score_iou_{iou_threshold}"] = round(f1_score, 4)
                    category_data[f"tp_iou_{iou_threshold}"] = metrics["tp"]
                    category_data[f"fp_iou_{iou_threshold}"] = metrics["fp"]
                    category_data[f"fn_iou_{iou_threshold}"] = metrics["fn"]

                # ë¶„ë¥˜ ì„±ëŠ¥ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
                if (
                    classification_results
                    and "class_wise_metrics" in classification_results
                ):
                    class_wise = classification_results["class_wise_metrics"].get(
                        category, {}
                    )
                    category_data["classification_precision"] = round(
                        class_wise.get("precision", 0.0), 4
                    )
                    category_data["classification_recall"] = round(
                        class_wise.get("recall", 0.0), 4
                    )
                    category_data["classification_f1"] = round(
                        class_wise.get("f1_score", 0.0), 4
                    )
                    category_data["classification_support"] = class_wise.get(
                        "support", 0
                    )
                    category_data["gt_count"] = class_wise.get("gt_count", 0)
                    category_data["pred_count"] = class_wise.get("pred_count", 0)

                category_summary.append(category_data)

            # CSV ì €ì¥
            if category_summary:
                summary_df = pd.DataFrame(category_summary)
                summary_csv_path = os.path.join(
                    output_dir, f"category_performance_summary_{timestamp}.csv"
                )
                summary_df.to_csv(summary_csv_path, index=False, encoding="utf-8-sig")
                print(f"ğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ìš”ì•½ ì €ì¥ (AP í¬í•¨): {summary_csv_path}")

        # 4. ê·¸ë£¹ë³„ ì„±ëŠ¥ ìš”ì•½ CSV (AP í‰ê·  í¬í•¨)
        if map_results and "map_metrics" in map_results:
            group_summary = []

            for group_name, categories in CATEGORY_GROUPS.items():
                group_data = {
                    "group": group_name,
                    "num_categories": len(categories),
                    "categories": ", ".join(categories),
                }

                # IoUë³„ ê·¸ë£¹ í‰ê·  ì„±ëŠ¥ ê³„ì‚° (AP í¬í•¨)
                for iou_key, iou_data in map_results["map_metrics"].items():
                    iou_threshold = iou_key.replace("iou_", "")
                    class_metrics = iou_data.get("class_metrics", {})

                    group_aps = []
                    group_precisions = []
                    group_recalls = []
                    group_f1s = []
                    group_tps = []
                    group_fps = []
                    group_fns = []

                    for category in categories:
                        if category in class_metrics:
                            metrics = class_metrics[category]
                        else:
                            metrics = {
                                "precision": 0.0,
                                "recall": 0.0,
                                "ap": 0.0,
                                "tp": 0,
                                "fp": 0,
                                "fn": 0,
                            }

                        f1_score = (
                            2
                            * metrics["precision"]
                            * metrics["recall"]
                            / (metrics["precision"] + metrics["recall"])
                            if (metrics["precision"] + metrics["recall"]) > 0
                            else 0
                        )

                        group_aps.append(metrics["ap"])  # **AP ê°’ ì¶”ê°€**
                        group_precisions.append(metrics["precision"])
                        group_recalls.append(metrics["recall"])
                        group_f1s.append(f1_score)
                        group_tps.append(metrics["tp"])
                        group_fps.append(metrics["fp"])
                        group_fns.append(metrics["fn"])

                    # ê·¸ë£¹ í‰ê·  ê³„ì‚° (AP í¬í•¨)
                    group_data[f"avg_ap_iou_{iou_threshold}"] = round(
                        np.mean(group_aps), 4
                    )  # **í‰ê·  AP ì¶”ê°€**
                    group_data[f"avg_precision_iou_{iou_threshold}"] = round(
                        np.mean(group_precisions), 4
                    )
                    group_data[f"avg_recall_iou_{iou_threshold}"] = round(
                        np.mean(group_recalls), 4
                    )
                    group_data[f"avg_f1_score_iou_{iou_threshold}"] = round(
                        np.mean(group_f1s), 4
                    )
                    group_data[f"total_tp_iou_{iou_threshold}"] = sum(group_tps)
                    group_data[f"total_fp_iou_{iou_threshold}"] = sum(group_fps)
                    group_data[f"total_fn_iou_{iou_threshold}"] = sum(group_fns)

                group_summary.append(group_data)

            # ê·¸ë£¹ë³„ ìš”ì•½ CSV ì €ì¥
            if group_summary:
                group_df = pd.DataFrame(group_summary)
                group_csv_path = os.path.join(
                    output_dir, f"group_performance_summary_{timestamp}.csv"
                )
                group_df.to_csv(group_csv_path, index=False, encoding="utf-8-sig")
                print(f"ğŸ“Š ê·¸ë£¹ë³„ ì„±ëŠ¥ ìš”ì•½ ì €ì¥ (AP í¬í•¨): {group_csv_path}")

        return {
            "comprehensive_metrics_json": metrics_path,
            "detailed_results_csv": detailed_csv_path if detailed_results else None,
            "category_performance_csv": (
                summary_csv_path if "summary_csv_path" in locals() else None
            ),
            "group_performance_csv": (
                group_csv_path if "group_csv_path" in locals() else None
            ),
        }

    def print_comprehensive_report(self, map_results, classification_results):
        """ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì¶œë ¥ (ì‹¤ì œ mAP í¬í•¨)"""
        print("\n" + "=" * 100)
        print("ğŸ¯ YOLO ì¢…í•© ì„±ëŠ¥ ê²€ì¦ ë¦¬í¬íŠ¸ (ê°ì²´ íƒì§€ + ë¶„ë¥˜)")
        print("=" * 100)

        # ê°ì²´ íƒì§€ ì„±ëŠ¥ (ì‹¤ì œ mAP)
        if map_results:
            print(f"\nğŸ¯ ê°ì²´ íƒì§€ ì„±ëŠ¥ (ì‹¤ì œ mAP):")
            print(f"  - mAP@0.5: {map_results.get('map_50', 0):.4f}")
            print(f"  - mAP@0.75: {map_results.get('map_75', 0):.4f}")
            print(f"  - mAP@0.5:0.95: {map_results.get('map_50_95', 0):.4f}")
            print(f"  - ì´ GT ê°ì²´ ìˆ˜: {map_results.get('total_gt_objects', 0):,}")
            print(f"  - ì´ ì˜ˆì¸¡ ê°ì²´ ìˆ˜: {map_results.get('total_pred_objects', 0):,}")

            # IoU@0.5ì—ì„œì˜ ì „ì²´ ì„±ëŠ¥
            if "map_metrics" in map_results and "iou_0.5" in map_results["map_metrics"]:
                iou_50_metrics = map_results["map_metrics"]["iou_0.5"]
                print(f"\nğŸ“Š íƒì§€ ì„±ëŠ¥ @IoU=0.5:")
                print(f"  - Precision: {iou_50_metrics.get('precision', 0):.4f}")
                print(f"  - Recall: {iou_50_metrics.get('recall', 0):.4f}")
                print(f"  - F1-Score: {iou_50_metrics.get('f1_score', 0):.4f}")
                print(f"  - Mean AP: {iou_50_metrics.get('mean_ap', 0):.4f}")
                print(
                    f"  - TP: {iou_50_metrics.get('tp', 0)}, FP: {iou_50_metrics.get('fp', 0)}, FN: {iou_50_metrics.get('fn', 0)}"
                )

            # ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ AP ê°’ ì¶œë ¥ (IoU@0.5) - ëª¨ë“  21ê°œ ì¹´í…Œê³ ë¦¬ í‘œì‹œ
            if "map_metrics" in map_results and "iou_0.5" in map_results["map_metrics"]:
                iou_50_metrics = map_results["map_metrics"]["iou_0.5"]
                class_metrics = iou_50_metrics.get("class_metrics", {})

                print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ mAP ì„±ëŠ¥ @IoU=0.5:")
                print(
                    f"{'ì¹´í…Œê³ ë¦¬':<15} {'AP':<8} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'TP':<5} {'FP':<5} {'FN':<5}"
                )
                print("-" * 85)

                # ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ë³„ë¡œ ì •ë¦¬ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬í•¨)
                for group_name, categories in CATEGORY_GROUPS.items():
                    print(f"\nğŸ“‚ {group_name}:")
                    for category in categories:
                        if category in class_metrics:
                            metrics = class_metrics[category]
                        else:
                            # ë°ì´í„°ê°€ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ë„ í‘œì‹œ
                            metrics = {
                                "precision": 0.0,
                                "recall": 0.0,
                                "ap": 0.0,
                                "tp": 0,
                                "fp": 0,
                                "fn": 0,
                            }

                        ap_value = metrics.get("ap", 0.0)
                        f1_score = (
                            2
                            * metrics["precision"]
                            * metrics["recall"]
                            / (metrics["precision"] + metrics["recall"])
                            if (metrics["precision"] + metrics["recall"]) > 0
                            else 0
                        )

                        print(
                            f"  {category:<13} {ap_value:<8.4f} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} "
                            f"{f1_score:<10.4f} {metrics['tp']:<5} {metrics['fp']:<5} {metrics['fn']:<5}"
                        )

            # ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ AP ê°’ ì¶œë ¥ (IoU@0.75)
            if (
                "map_metrics" in map_results
                and "iou_0.75" in map_results["map_metrics"]
            ):
                iou_75_metrics = map_results["map_metrics"]["iou_0.75"]
                class_metrics = iou_75_metrics.get("class_metrics", {})

                print(f"\nğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ mAP ì„±ëŠ¥ @IoU=0.75:")
                print(
                    f"{'ì¹´í…Œê³ ë¦¬':<15} {'AP':<8} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'TP':<5} {'FP':<5} {'FN':<5}"
                )
                print("-" * 85)

                # ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ ìˆœì„œëŒ€ë¡œ í‘œì‹œ
                for category in CATEGORIES:
                    if category in class_metrics:
                        metrics = class_metrics[category]
                    else:
                        metrics = {
                            "precision": 0.0,
                            "recall": 0.0,
                            "ap": 0.0,
                            "tp": 0,
                            "fp": 0,
                            "fn": 0,
                        }

                    ap_value = metrics.get("ap", 0.0)
                    f1_score = (
                        2
                        * metrics["precision"]
                        * metrics["recall"]
                        / (metrics["precision"] + metrics["recall"])
                        if (metrics["precision"] + metrics["recall"]) > 0
                        else 0
                    )

                    print(
                        f"  {category:<13} {ap_value:<8.4f} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} "
                        f"{f1_score:<10.4f} {metrics['tp']:<5} {metrics['fp']:<5} {metrics['fn']:<5}"
                    )

        # ë¶„ë¥˜ ì„±ëŠ¥ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
        if (
            classification_results
            and "classification_metrics" in classification_results
        ):
            cls_metrics = classification_results["classification_metrics"]
            print(f"\nğŸ·ï¸ ë¶„ë¥˜ ì„±ëŠ¥:")
            print(f"  - ì •í™•ë„: {cls_metrics.get('accuracy', 0):.4f}")
            print(f"  - ê· í˜• ì •í™•ë„: {cls_metrics.get('balanced_accuracy', 0):.4f}")
            print(f"  - Macro F1: {cls_metrics.get('macro_f1', 0):.4f}")
            print(f"  - Weighted F1: {cls_metrics.get('weighted_f1', 0):.4f}")

            # ì‹ ë¢°ë„ í†µê³„
            if "confidence_stats" in classification_results:
                conf_stats = classification_results["confidence_stats"]
                print(f"\nğŸ” ì˜ˆì¸¡ ì‹ ë¢°ë„:")
                print(f"  - í‰ê· : {conf_stats.get('mean_confidence', 0):.4f}")
                print(f"  - í‘œì¤€í¸ì°¨: {conf_stats.get('std_confidence', 0):.4f}")
                print(
                    f"  - ë²”ìœ„: [{conf_stats.get('min_confidence', 0):.4f}, {conf_stats.get('max_confidence', 0):.4f}]"
                )

        # ìµœê³ /ìµœì € ì„±ëŠ¥ í´ë˜ìŠ¤ (ë¶„ë¥˜ ê¸°ì¤€)
        if classification_results and "class_wise_metrics" in classification_results:
            class_metrics = classification_results["class_wise_metrics"]
            class_f1_scores = {
                k: v["f1_score"] for k, v in class_metrics.items() if v["support"] > 0
            }

            if class_f1_scores:
                best_class = max(class_f1_scores, key=class_f1_scores.get)
                worst_class = min(class_f1_scores, key=class_f1_scores.get)

                print(f"\nğŸ† í´ë˜ìŠ¤ë³„ ë¶„ë¥˜ ì„±ëŠ¥:")
                print(
                    f"  - ìµœê³  ì„±ëŠ¥: {best_class} (F1: {class_f1_scores[best_class]:.4f})"
                )
                print(
                    f"  - ìµœì € ì„±ëŠ¥: {worst_class} (F1: {class_f1_scores[worst_class]:.4f})"
                )

        # mAPì—ì„œ ìµœê³ /ìµœì € ì„±ëŠ¥ ì¹´í…Œê³ ë¦¬ (AP ê¸°ì¤€)
        if (
            map_results
            and "map_metrics" in map_results
            and "iou_0.5" in map_results["map_metrics"]
        ):
            iou_50_metrics = map_results["map_metrics"]["iou_0.5"]
            if "class_aps" in iou_50_metrics:
                class_aps = iou_50_metrics["class_aps"]

                if class_aps:
                    best_detection_class = max(class_aps, key=class_aps.get)
                    worst_detection_class = min(class_aps, key=class_aps.get)

                    print(f"\nğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ íƒì§€ ì„±ëŠ¥ (AP@0.5):")
                    print(
                        f"  - ìµœê³  ì„±ëŠ¥: {best_detection_class} (AP: {class_aps[best_detection_class]:.4f})"
                    )
                    print(
                        f"  - ìµœì € ì„±ëŠ¥: {worst_detection_class} (AP: {class_aps[worst_detection_class]:.4f})"
                    )

        print("=" * 100)

    def print_detailed_category_analysis(self, map_results):
        """ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„ ì¶œë ¥ (AP ê¸°ë°˜)"""
        if not map_results or "map_metrics" not in map_results:
            return

        print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ mAP ë¶„ì„:")

        for iou_key, iou_data in map_results["map_metrics"].items():
            iou_threshold = iou_key.replace("iou_", "")
            print(f"\nğŸ¯ IoU Threshold: {iou_threshold}")

            if "class_aps" not in iou_data:
                continue

            # ê·¸ë£¹ë³„ í‰ê·  AP ê³„ì‚°
            group_performances = {}
            for group_name, categories in CATEGORY_GROUPS.items():
                group_aps = []
                group_precisions = []
                group_recalls = []

                for category in categories:
                    if category in iou_data.get("class_aps", {}):
                        ap = iou_data["class_aps"][category]
                        metrics = iou_data.get("class_metrics", {}).get(category, {})
                        precision = metrics.get("precision", 0.0)
                        recall = metrics.get("recall", 0.0)
                    else:
                        ap = 0.0
                        precision = 0.0
                        recall = 0.0

                    group_aps.append(ap)
                    group_precisions.append(precision)
                    group_recalls.append(recall)

                if group_aps:
                    group_performances[group_name] = {
                        "avg_ap": np.mean(group_aps),
                        "avg_precision": np.mean(group_precisions),
                        "avg_recall": np.mean(group_recalls),
                        "num_categories": len(group_aps),
                    }

            # ê·¸ë£¹ë³„ ì„±ëŠ¥ ì¶œë ¥ (AP ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬)
            if group_performances:
                print(f"\nğŸ“‚ ê·¸ë£¹ë³„ í‰ê·  ì„±ëŠ¥:")
                for group_name, perf in sorted(
                    group_performances.items(),
                    key=lambda x: x[1]["avg_ap"],
                    reverse=True,
                ):
                    print(
                        f"  {group_name:<10} - AP: {perf['avg_ap']:.4f}, "
                        f"Precision: {perf['avg_precision']:.4f}, "
                        f"Recall: {perf['avg_recall']:.4f} "
                        f"({perf['num_categories']}ê°œ ì¹´í…Œê³ ë¦¬)"
                    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ YOLO ì¢…í•© ì„±ëŠ¥ ê²€ì¦ ì‹œì‘ (ê°ì²´ íƒì§€ + ë¶„ë¥˜, ì‹¤ì œ mAP í¬í•¨)")

    # ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
    for path, name in [
        (IMAGE_DIR, "ì´ë¯¸ì§€"),
        (LABEL_DIR, "ë¼ë²¨"),
        (MODEL_PATH, "ëª¨ë¸"),
    ]:
        if not os.path.exists(path):
            print(f"âŒ {name} ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
            return

    try:
        # ê²€ì¦ê¸° ì´ˆê¸°í™”
        validator = YOLODetectionValidator(MODEL_PATH)

        # Ground Truth ë°ì´í„° ë¡œë“œ
        print(f"\nğŸ“‚ Ground Truth ë°ì´í„° ë¡œë”©...")
        gt_data = validator.load_ground_truth_data(IMAGE_DIR, LABEL_DIR)

        if len(gt_data) == 0:
            print("âŒ ìœ íš¨í•œ Ground Truth ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # YOLO ì˜ˆì¸¡ ìˆ˜í–‰
        print(f"\nğŸ”® YOLO ì˜ˆì¸¡ ìˆ˜í–‰...")
        predictions = validator.predict_with_boxes(IMAGE_DIR, gt_data)

        # ì‹¤ì œ mAP ë©”íŠ¸ë¦­ ê³„ì‚°
        print(f"\nğŸ“Š ì‹¤ì œ mAP ë©”íŠ¸ë¦­ ê³„ì‚°...")
        map_results = validator.calculate_map_metrics(gt_data, predictions)

        # ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°
        print(f"\nğŸ·ï¸ ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚°...")
        classification_results = validator.calculate_classification_metrics(
            gt_data, predictions
        )

        # ê²°ê³¼ ì €ì¥
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥...")
        saved_files = validator.save_comprehensive_results(
            map_results, classification_results, predictions, gt_data, RESULTS_DIR
        )

        # ì¢…í•© ë¦¬í¬íŠ¸ ì¶œë ¥ (ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ì œ mAP í¬í•¨)
        validator.print_comprehensive_report(map_results, classification_results)

        # ìƒì„¸ ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì¶œë ¥ (AP ê¸°ë°˜)
        validator.print_detailed_category_analysis(map_results)

        print(f"\nâœ… ì¢…í•© ê²€ì¦ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤ì´ ì €ì¥ëœ ìœ„ì¹˜: {RESULTS_DIR}")
        for file_type, file_path in saved_files.items():
            if file_path:
                print(f"  - {file_type}: {os.path.basename(file_path)}")

    except Exception as e:
        print(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
